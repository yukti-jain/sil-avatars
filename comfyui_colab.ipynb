{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaaaaaaaaa"
      },
      "source": [
        "Git clone the repo and install the requirements. (ignore the pip errors about protobuf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbbbbbbbbb",
        "outputId": "52722575-3161-48d4-92c1-d4fb810e1b1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounting Google Drive...\n",
            "/\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive\n",
            "/content/drive/MyDrive/ComfyUI\n",
            "-= Updating ComfyUI =-\n",
            "Updating eeca724..719fb2c\n",
            "error: Your local changes to the following files would be overwritten by merge:\n",
            "\t.github/workflows/windows_release_cu118_dependencies.yml\n",
            "\t.github/workflows/windows_release_cu118_dependencies_2.yml\n",
            "\t.github/workflows/windows_release_cu118_package.yml\n",
            "\t.github/workflows/windows_release_dependencies.yml\n",
            "\t.github/workflows/windows_release_nightly_pytorch.yml\n",
            "\t.github/workflows/windows_release_package.yml\n",
            "\tREADME.md\n",
            "\tcomfy/cli_args.py\n",
            "\tcomfy/clip_model.py\n",
            "\tcomfy/clip_vision.py\n",
            "\tcomfy/controlnet.py\n",
            "\tcomfy/diffusers_convert.py\n",
            "\tcomfy/extra_samplers/uni_pc.py\n",
            "\tcomfy/gligen.py\n",
            "\tcomfy/k_diffusion/sampling.py\n",
            "\tcomfy/latent_formats.py\n",
            "\tcomfy/ldm/modules/attention.py\n",
            "\tcomfy/ldm/modules/diffusionmodules/model.py\n",
            "\tcomfy/ldm/modules/diffusionmodules/openaimodel.py\n",
            "\tcomfy/ldm/modules/diffusionmodules/util.py\n",
            "\tcomfy/ldm/modules/sub_quadratic_attention.py\n",
            "\tcomfy/lora.py\n",
            "\tcomfy/model_base.py\n",
            "\tcomfy/model_detection.py\n",
            "\tcomfy/model_management.py\n",
            "\tcomfy/model_patcher.py\n",
            "\tcomfy/model_sampling.py\n",
            "\tcomfy/ops.py\n",
            "\tcomfy/sample.py\n",
            "\tcomfy/samplers.py\n",
            "\tcomfy/sd.py\n",
            "\tcomfy/sd1_clip.py\n",
            "\tcomfy/sdxl_clip.py\n",
            "\tcomfy/supported_models.py\n",
            "\tcomfy/supported_models_base.py\n",
            "\tcomfy/utils.py\n",
            "\tcomfy_extras/nodes_canny.py\n",
            "\tcomfy_extras/nodes_clip_sdxl.py\n",
            "\tcomfy_extras/nodes_custom_sampler.py\n",
            "\tcomfy_extras/nodes_freelunch.py\n",
            "\tcomfy_extras/nodes_hypernetwork.py\n",
            "\tcomfy_extras/nodes_images.py\n",
            "\tcomfy_extras/nodes_latent.py\n",
            "\tcomfy_extras/nodes_mask.py\n",
            "\tcomfy_extras/nodes_model_advanced.py\n",
            "\tcomfy_extras/nodes_model_merging.py\n",
            "\tcomfy_extras/nodes_perpneg.py\n",
            "\tcomfy_extras/nodes_photomaker.py\n",
            "\tcomfy_extras/nodes_post_processing.py\n",
            "\tcomfy_extras/nodes_sag.py\n",
            "\tcomfy_extras/nodes_stable3d.py\n",
            "\tcomfy_extras/nodes_video_model.py\n",
            "\tcuda_malloc.py\n",
            "\tcustom_nodes/example_node.py.example\n",
            "\texecution.py\n",
            "\tfolder_paths.py\n",
            "\tlatent_preview.py\n",
            "\tmain.py\n",
            "\tnodes.py\n",
            "\trequirements.txt\n",
            "\tserver.py\n",
            "\ttests-ui/tests/groupNode.test.js\n",
            "\ttests-ui/utils/ezgraph.js\n",
            "\tweb/extensions/core/colorPalette.js\n",
            "\tweb/extensions/core/dynamicPrompts.js\n",
            "\tweb/extensions/core/maskeditor.js\n",
            "\tweb/extensions/core/widgetInputs.js\n",
            "\tweb/lib/litegraph.core.js\n",
            "\tweb/lib/litegraph.css\n",
            "\tweb/scripts/app.js\n",
            "\tweb/scripts/pnginfo.js\n",
            "\tweb/scripts/ui.js\n",
            "\tweb/scripts/ui/settings.js\n",
            "\tweb/scripts/widgets.js\n",
            "\tweb/style.css\n",
            "Please commit your changes or stash them before you merge.\n",
            "Aborting\n",
            "-= Install dependencies =-\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu118, https://download.pytorch.org/whl/cu117\n",
            "Requirement already satisfied: xformers!=0.0.18 in /usr/local/lib/python3.10/dist-packages (0.0.25.post1+cu118)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.2.2+cu118)\n",
            "Requirement already satisfied: torchsde in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (0.2.6)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (0.17.2+cu118)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (0.7.0)\n",
            "Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (4.38.2)\n",
            "Requirement already satisfied: safetensors>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (0.4.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (3.9.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (6.0.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (9.4.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (1.11.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (4.66.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (5.9.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers!=0.0.18) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.7.0.84 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (8.7.0.84)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (11.8.86)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (2.2.0)\n",
            "Requirement already satisfied: trampoline>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from torchsde->-r requirements.txt (line 2)) (0.1.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->-r requirements.txt (line 5)) (0.20.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->-r requirements.txt (line 5)) (24.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->-r requirements.txt (line 5)) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->-r requirements.txt (line 5)) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->-r requirements.txt (line 5)) (0.15.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->-r requirements.txt (line 7)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->-r requirements.txt (line 7)) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->-r requirements.txt (line 7)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->-r requirements.txt (line 7)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->-r requirements.txt (line 7)) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->-r requirements.txt (line 7)) (4.0.3)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp->-r requirements.txt (line 7)) (3.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 1)) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.25.1->-r requirements.txt (line 5)) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.25.1->-r requirements.txt (line 5)) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.25.1->-r requirements.txt (line 5)) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r requirements.txt (line 1)) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "#@title Environment Setup\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "OPTIONS = {}\n",
        "\n",
        "USE_GOOGLE_DRIVE = True  #@param {type:\"boolean\"}\n",
        "UPDATE_COMFY_UI = True  #@param {type:\"boolean\"}\n",
        "WORKSPACE = 'ComfyUI'\n",
        "OPTIONS['USE_GOOGLE_DRIVE'] = USE_GOOGLE_DRIVE\n",
        "OPTIONS['UPDATE_COMFY_UI'] = UPDATE_COMFY_UI\n",
        "\n",
        "if OPTIONS['USE_GOOGLE_DRIVE']:\n",
        "    !echo \"Mounting Google Drive...\"\n",
        "    %cd /\n",
        "\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    WORKSPACE = \"/content/drive/MyDrive/ComfyUI\"\n",
        "    %cd /content/drive/MyDrive\n",
        "\n",
        "![ ! -d $WORKSPACE ] && echo -= Initial setup ComfyUI =- && git clone https://github.com/comfyanonymous/ComfyUI\n",
        "%cd $WORKSPACE\n",
        "\n",
        "if OPTIONS['UPDATE_COMFY_UI']:\n",
        "  !echo -= Updating ComfyUI =-\n",
        "  !git pull\n",
        "\n",
        "!echo -= Install dependencies =-\n",
        "!pip install xformers!=0.0.18 -r requirements.txt --extra-index-url https://download.pytorch.org/whl/cu118 --extra-index-url https://download.pytorch.org/whl/cu117"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDRKkTPpUE33",
        "outputId": "ffa35360-d3f2-4a0c-bfb3-3077e5dc8ded"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cccccccccc"
      },
      "source": [
        "Download some models/checkpoints/vae or custom comfyui nodes (uncomment the commands for the ones you want)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dddddddddd",
        "outputId": "9486993a-d4f3-4b6d-c550-01592283e9ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-03-12 20:51:03--  https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.ckpt\n",
            "Resolving huggingface.co (huggingface.co)... 18.239.50.103, 18.239.50.80, 18.239.50.16, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.239.50.103|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/6b/20/6b201da5f0f5c60524535ebb7deac2eef68605655d3bbacfee9cce0087f3b3f5/cc6cb27103417325ff94f52b7a5d2dde45a7515b25c255d8e396c90014281516?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27v1-5-pruned-emaonly.ckpt%3B+filename%3D%22v1-5-pruned-emaonly.ckpt%22%3B&Expires=1710535864&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxMDUzNTg2NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy82Yi8yMC82YjIwMWRhNWYwZjVjNjA1MjQ1MzVlYmI3ZGVhYzJlZWY2ODYwNTY1NWQzYmJhY2ZlZTljY2UwMDg3ZjNiM2Y1L2NjNmNiMjcxMDM0MTczMjVmZjk0ZjUyYjdhNWQyZGRlNDVhNzUxNWIyNWMyNTVkOGUzOTZjOTAwMTQyODE1MTY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=togP8Ze2z%7ER6et0-qNfUzAIeFrTYJMoHEAprMiSztxPHW0AaSHi3X4Y%7EmnEJ28ZSxG3oSvDz5Kpw9x-N-EYltB6gJBP9cfKBd2l7ekJPgXpTN8BOkK4AbQQGooBRPgKkwM6mQWWxaBppxkvfoKPr%7E0oFuXIaVUTxf3LTcw0ioy98zpw5CqFI58zGedDQklBLAAXIpyfwIIRtafqQSQGrIzpJc0e84rIRpFsWweMZVlP0nG-5lwN8gCbabLjnYur0mQx2h0UPGmGkIQJaN4q4IwUjhC-a-iCEMYJbuiRqYpnChJ1iIa-Zf6B-B%7EzAB2eBBv5jPd8xv9woEJx7oVqy-A__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2024-03-12 20:51:04--  https://cdn-lfs.huggingface.co/repos/6b/20/6b201da5f0f5c60524535ebb7deac2eef68605655d3bbacfee9cce0087f3b3f5/cc6cb27103417325ff94f52b7a5d2dde45a7515b25c255d8e396c90014281516?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27v1-5-pruned-emaonly.ckpt%3B+filename%3D%22v1-5-pruned-emaonly.ckpt%22%3B&Expires=1710535864&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxMDUzNTg2NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy82Yi8yMC82YjIwMWRhNWYwZjVjNjA1MjQ1MzVlYmI3ZGVhYzJlZWY2ODYwNTY1NWQzYmJhY2ZlZTljY2UwMDg3ZjNiM2Y1L2NjNmNiMjcxMDM0MTczMjVmZjk0ZjUyYjdhNWQyZGRlNDVhNzUxNWIyNWMyNTVkOGUzOTZjOTAwMTQyODE1MTY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=togP8Ze2z%7ER6et0-qNfUzAIeFrTYJMoHEAprMiSztxPHW0AaSHi3X4Y%7EmnEJ28ZSxG3oSvDz5Kpw9x-N-EYltB6gJBP9cfKBd2l7ekJPgXpTN8BOkK4AbQQGooBRPgKkwM6mQWWxaBppxkvfoKPr%7E0oFuXIaVUTxf3LTcw0ioy98zpw5CqFI58zGedDQklBLAAXIpyfwIIRtafqQSQGrIzpJc0e84rIRpFsWweMZVlP0nG-5lwN8gCbabLjnYur0mQx2h0UPGmGkIQJaN4q4IwUjhC-a-iCEMYJbuiRqYpnChJ1iIa-Zf6B-B%7EzAB2eBBv5jPd8xv9woEJx7oVqy-A__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.239.18.68, 18.239.18.29, 18.239.18.94, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.239.18.68|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4265380512 (4.0G) [binary/octet-stream]\n",
            "Saving to: ‘./models/checkpoints/v1-5-pruned-emaonly.ckpt’\n",
            "\n",
            "v1-5-pruned-emaonly 100%[===================>]   3.97G  57.6MB/s    in 75s     \n",
            "\n",
            "2024-03-12 20:52:19 (54.1 MB/s) - ‘./models/checkpoints/v1-5-pruned-emaonly.ckpt’ saved [4265380512/4265380512]\n",
            "\n",
            "--2024-03-12 20:52:19--  https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors\n",
            "Resolving huggingface.co (huggingface.co)... 18.239.50.103, 18.239.50.16, 18.239.50.49, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.239.50.103|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/ec/ee/eceee26c5834d8a75cf04eeb17dfc06d1d5fe1d80c2f19520b148c11e2e98c45/735e4c3a447a3255760d7f86845f09f937809baa529c17370d83e4c3758f3c75?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27vae-ft-mse-840000-ema-pruned.safetensors%3B+filename%3D%22vae-ft-mse-840000-ema-pruned.safetensors%22%3B&Expires=1710533743&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxMDUzMzc0M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9lYy9lZS9lY2VlZTI2YzU4MzRkOGE3NWNmMDRlZWIxN2RmYzA2ZDFkNWZlMWQ4MGMyZjE5NTIwYjE0OGMxMWUyZTk4YzQ1LzczNWU0YzNhNDQ3YTMyNTU3NjBkN2Y4Njg0NWYwOWY5Mzc4MDliYWE1MjljMTczNzBkODNlNGMzNzU4ZjNjNzU%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=RWKUXQXLs%7ErSMToSb0V4SKlNP6VKcKFg0Zb%7E4xmMThM5TawtccV-m78eRVo-LFeHxiapSNy%7EQ1ZuZZbo2qZfy5yZFOX35TmXGgZYwGdSRRwWXCSCyYRH3gLavhYhl6RadN7uEcWDfYdBXEdcCkw0snOeYau6q5Lcn5242VFZLIxv9oSOV9BDZQ4z5gXsGlWhfo7ovA6%7ED1Kn2JJG3WZUYjmR7m4gHh6yYMQsy7fLUNYN4jKSKztf2-FP4kNjWCF%7EZ4dmK5wHW7bCiJ-LuKH58JwzccYTwZLEVz%7Eo39k91t9i8npaXB6L2QXzwljdy77kf83LcPOiJa7AiZoC7dAIQQ__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2024-03-12 20:52:20--  https://cdn-lfs.huggingface.co/repos/ec/ee/eceee26c5834d8a75cf04eeb17dfc06d1d5fe1d80c2f19520b148c11e2e98c45/735e4c3a447a3255760d7f86845f09f937809baa529c17370d83e4c3758f3c75?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27vae-ft-mse-840000-ema-pruned.safetensors%3B+filename%3D%22vae-ft-mse-840000-ema-pruned.safetensors%22%3B&Expires=1710533743&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxMDUzMzc0M319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9lYy9lZS9lY2VlZTI2YzU4MzRkOGE3NWNmMDRlZWIxN2RmYzA2ZDFkNWZlMWQ4MGMyZjE5NTIwYjE0OGMxMWUyZTk4YzQ1LzczNWU0YzNhNDQ3YTMyNTU3NjBkN2Y4Njg0NWYwOWY5Mzc4MDliYWE1MjljMTczNzBkODNlNGMzNzU4ZjNjNzU%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=RWKUXQXLs%7ErSMToSb0V4SKlNP6VKcKFg0Zb%7E4xmMThM5TawtccV-m78eRVo-LFeHxiapSNy%7EQ1ZuZZbo2qZfy5yZFOX35TmXGgZYwGdSRRwWXCSCyYRH3gLavhYhl6RadN7uEcWDfYdBXEdcCkw0snOeYau6q5Lcn5242VFZLIxv9oSOV9BDZQ4z5gXsGlWhfo7ovA6%7ED1Kn2JJG3WZUYjmR7m4gHh6yYMQsy7fLUNYN4jKSKztf2-FP4kNjWCF%7EZ4dmK5wHW7bCiJ-LuKH58JwzccYTwZLEVz%7Eo39k91t9i8npaXB6L2QXzwljdy77kf83LcPOiJa7AiZoC7dAIQQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.239.18.84, 18.239.18.68, 18.239.18.94, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.239.18.84|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 334641190 (319M) [binary/octet-stream]\n",
            "Saving to: ‘./models/vae/vae-ft-mse-840000-ema-pruned.safetensors’\n",
            "\n",
            "vae-ft-mse-840000-e 100%[===================>] 319.14M  54.8MB/s    in 5.9s    \n",
            "\n",
            "2024-03-12 20:52:26 (54.2 MB/s) - ‘./models/vae/vae-ft-mse-840000-ema-pruned.safetensors’ saved [334641190/334641190]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Checkpoints\n",
        "\n",
        "# SD1.5\n",
        "!wget -c https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.ckpt -P ./models/checkpoints/\n",
        "\n",
        "# SD2\n",
        "#!wget -c https://huggingface.co/stabilityai/stable-diffusion-2-1-base/resolve/main/v2-1_512-ema-pruned.safetensors -P ./models/checkpoints/\n",
        "#!wget -c https://huggingface.co/stabilityai/stable-diffusion-2-1/resolve/main/v2-1_768-ema-pruned.safetensors -P ./models/checkpoints/\n",
        "\n",
        "# Some SD1.5 anime style\n",
        "#!wget -c https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/Models/AbyssOrangeMix2/AbyssOrangeMix2_hard.safetensors -P ./models/checkpoints/\n",
        "#!wget -c https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/Models/AbyssOrangeMix3/AOM3A1_orangemixs.safetensors -P ./models/checkpoints/\n",
        "#!wget -c https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/Models/AbyssOrangeMix3/AOM3A3_orangemixs.safetensors -P ./models/checkpoints/\n",
        "#!wget -c https://huggingface.co/Linaqruf/anything-v3.0/resolve/main/anything-v3-fp16-pruned.safetensors -P ./models/checkpoints/\n",
        "\n",
        "# Waifu Diffusion 1.5 (anime style SD2.x 768-v)\n",
        "#!wget -c https://huggingface.co/waifu-diffusion/wd-1-5-beta2/resolve/main/checkpoints/wd-1-5-beta2-fp16.safetensors -P ./models/checkpoints/\n",
        "\n",
        "\n",
        "# unCLIP models\n",
        "#!wget -c https://huggingface.co/comfyanonymous/illuminatiDiffusionV1_v11_unCLIP/resolve/main/illuminatiDiffusionV1_v11-unclip-h-fp16.safetensors -P ./models/checkpoints/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/wd-1.5-beta2_unCLIP/resolve/main/wd-1-5-beta2-aesthetic-unclip-h-fp16.safetensors -P ./models/checkpoints/\n",
        "\n",
        "\n",
        "# VAE\n",
        "!wget -c https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors -P ./models/vae/\n",
        "#!wget -c https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/VAEs/orangemix.vae.pt -P ./models/vae/\n",
        "#!wget -c https://huggingface.co/hakurei/waifu-diffusion-v1-4/resolve/main/vae/kl-f8-anime2.ckpt -P ./models/vae/\n",
        "\n",
        "\n",
        "# Loras\n",
        "#!wget -c https://civitai.com/api/download/models/10350 -O ./models/loras/theovercomer8sContrastFix_sd21768.safetensors #theovercomer8sContrastFix SD2.x 768-v\n",
        "#!wget -c https://civitai.com/api/download/models/10638 -O ./models/loras/theovercomer8sContrastFix_sd15.safetensors #theovercomer8sContrastFix SD1.x\n",
        "\n",
        "\n",
        "# T2I-Adapter\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_depth_sd14v1.pth -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_seg_sd14v1.pth -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_sketch_sd14v1.pth -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_keypose_sd14v1.pth -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_openpose_sd14v1.pth -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_color_sd14v1.pth -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_canny_sd14v1.pth -P ./models/controlnet/\n",
        "\n",
        "# T2I Styles Model\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_style_sd14v1.pth -P ./models/style_models/\n",
        "\n",
        "# CLIPVision model (needed for styles model)\n",
        "#!wget -c https://huggingface.co/openai/clip-vit-large-patch14/resolve/main/pytorch_model.bin -O ./models/clip_vision/clip_vit14.bin\n",
        "\n",
        "\n",
        "# ControlNet\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11e_sd15_ip2p_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11e_sd15_shuffle_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_canny_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11f1p_sd15_depth_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_inpaint_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_lineart_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_mlsd_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_normalbae_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_openpose_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_scribble_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_seg_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_softedge_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15s2_lineart_anime_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11u_sd15_tile_fp16.safetensors -P ./models/controlnet/\n",
        "\n",
        "\n",
        "# Controlnet Preprocessor nodes by Fannovel16\n",
        "#!cd custom_nodes && git clone https://github.com/Fannovel16/comfy_controlnet_preprocessors; cd comfy_controlnet_preprocessors && python install.py\n",
        "\n",
        "\n",
        "# GLIGEN\n",
        "#!wget -c https://huggingface.co/comfyanonymous/GLIGEN_pruned_safetensors/resolve/main/gligen_sd14_textbox_pruned_fp16.safetensors -P ./models/gligen/\n",
        "\n",
        "\n",
        "# ESRGAN upscale model\n",
        "#!wget -c https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth -P ./models/upscale_models/\n",
        "#!wget -c https://huggingface.co/sberbank-ai/Real-ESRGAN/resolve/main/RealESRGAN_x2.pth -P ./models/upscale_models/\n",
        "#!wget -c https://huggingface.co/sberbank-ai/Real-ESRGAN/resolve/main/RealESRGAN_x4.pth -P ./models/upscale_models/\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkkkkkkkkkkkkk"
      },
      "source": [
        "### Run ComfyUI with localtunnel (Recommended Way)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjjjjjjjjjjjj",
        "outputId": "8b20746d-5837-41b1-8b60-d9ee2615f236"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K\u001b[?25h/tools/node/bin/lt -> /tools/node/lib/node_modules/localtunnel/bin/lt.js\n",
            "\u001b[K\u001b[?25h+ localtunnel@2.0.2\n",
            "added 22 packages from 22 contributors in 1.795s\n",
            "** ComfyUI startup time: 2024-04-19 02:08:37.699126\n",
            "** Platform: Linux\n",
            "** Python version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n",
            "** Python executable: /usr/bin/python3\n",
            "** Log path: /content/drive/MyDrive/ComfyUI/comfyui.log\n",
            "\n",
            "Prestartup times for custom nodes:\n",
            "   1.7 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/ComfyUI-Manager\n",
            "\n",
            "Total VRAM 15102 MB, total RAM 12979 MB\n",
            "xformers version: 0.0.25.post1+cu118\n",
            "Set vram state to: NORMAL_VRAM\n",
            "Device: cuda:0 Tesla T4 : cudaMallocAsync\n",
            "VAE dtype: torch.float32\n",
            "Using xformers cross attention\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/ComfyUI/nodes.py\", line 1893, in load_custom_node\n",
            "    module_spec.loader.exec_module(module)\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
            "  File \"/content/drive/MyDrive/ComfyUI/custom_nodes/comfy_controlnet_preprocessors/__init__.py\", line 1, in <module>\n",
            "    from custom_nodes.comfy_controlnet_preprocessors.nodes import edge_line, normal_depth_map, pose, semseg, others\n",
            "  File \"/content/drive/MyDrive/ComfyUI/custom_nodes/comfy_controlnet_preprocessors/__init__.py\", line 1, in <module>\n",
            "    from custom_nodes.comfy_controlnet_preprocessors.nodes import edge_line, normal_depth_map, pose, semseg, others\n",
            "  File \"/content/drive/MyDrive/ComfyUI/custom_nodes/comfy_controlnet_preprocessors/nodes/edge_line.py\", line 1, in <module>\n",
            "    from .util import common_annotator_call, img_np_to_tensor, img_tensor_to_np, skip_v1\n",
            "  File \"/content/drive/MyDrive/ComfyUI/custom_nodes/comfy_controlnet_preprocessors/nodes/util.py\", line 1, in <module>\n",
            "    from ..v1 import openpose_v1, midas\n",
            "  File \"/content/drive/MyDrive/ComfyUI/custom_nodes/comfy_controlnet_preprocessors/v1/midas/__init__.py\", line 6, in <module>\n",
            "    from .api import MiDaSInference\n",
            "  File \"/content/drive/MyDrive/ComfyUI/custom_nodes/comfy_controlnet_preprocessors/v1/midas/api.py\", line 9, in <module>\n",
            "    from .midas.dpt_depth import DPTDepthModel\n",
            "  File \"/content/drive/MyDrive/ComfyUI/custom_nodes/comfy_controlnet_preprocessors/v1/midas/midas/dpt_depth.py\", line 6, in <module>\n",
            "    from .blocks import (\n",
            "  File \"/content/drive/MyDrive/ComfyUI/custom_nodes/comfy_controlnet_preprocessors/v1/midas/midas/blocks.py\", line 4, in <module>\n",
            "    from .vit import (\n",
            "  File \"/content/drive/MyDrive/ComfyUI/custom_nodes/comfy_controlnet_preprocessors/v1/midas/midas/vit.py\", line 3, in <module>\n",
            "    import timm\n",
            "ModuleNotFoundError: No module named 'timm'\n",
            "\n",
            "Cannot import /content/drive/MyDrive/ComfyUI/custom_nodes/comfy_controlnet_preprocessors module for custom nodes: No module named 'timm'\n",
            "### Loading: ComfyUI-Manager (V2.10)\n",
            "## ComfyUI-Manager: installing dependencies\n",
            " Collecting GitPython (from -r /content/drive/MyDrive/ComfyUI/custom_nodes/ComfyUI-Manager/requirements.txt (line 1))\n",
            "   Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 207.3/207.3 kB 5.7 MB/s eta 0:00:00\n",
            " Collecting matrix-client==0.4.0 (from -r /content/drive/MyDrive/ComfyUI/custom_nodes/ComfyUI-Manager/requirements.txt (line 2))\n",
            "   Downloading matrix_client-0.4.0-py2.py3-none-any.whl (43 kB)\n",
            "      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.5/43.5 kB 5.7 MB/s eta 0:00:00\n",
            "    Collecting urllib3~=1.21 (from matrix-client==0.4.0->-r /content/drive/MyDrive/ComfyUI/custom_nodes/ComfyUI-Manager/requirements.txt (line 2))\n",
            "   Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 143.8/143.8 kB 14.2 MB/s eta 0:00:00\n",
            " Collecting gitdb<5,>=4.0.1 (from GitPython->-r /content/drive/MyDrive/ComfyUI/custom_nodes/ComfyUI-Manager/requirements.txt (line 1))\n",
            "   Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 8.3 MB/s eta 0:00:00\n",
            "           Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython->-r /content/drive/MyDrive/ComfyUI/custom_nodes/ComfyUI-Manager/requirements.txt (line 1))\n",
            "   Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "    Installing collected packages: urllib3, smmap, gitdb, matrix-client, GitPython\n",
            "   Attempting uninstall: urllib3\n",
            "     Found existing installation: urllib3 2.0.7\n",
            "     Uninstalling urllib3-2.0.7:\n",
            "       Successfully uninstalled urllib3-2.0.7\n",
            "[!] ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "[!]  torchtext 0.17.1 requires torch==2.2.1, but you have torch 2.2.2+cu118 which is incompatible.\n",
            "Successfully installed GitPython-3.1.43 gitdb-4.0.11 matrix-client-0.4.0 smmap-5.0.1 urllib3-1.26.18\n",
            "## ComfyUI-Manager: installing dependencies done.\n",
            "### ComfyUI Revision: 1943 [eeca7248] | Released on '2024-01-31'\n",
            "[ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/alter-list.json\n",
            "[ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/custom-node-list.json\n",
            "\u001b[36;20m[comfyui_controlnet_aux] | INFO -> Using ckpts path: /content/drive/MyDrive/ComfyUI/custom_nodes/comfyui_controlnet_aux/ckpts\u001b[0m\n",
            "\u001b[36;20m[comfyui_controlnet_aux] | INFO -> Using symlinks: False\u001b[0m\n",
            "\u001b[36;20m[comfyui_controlnet_aux] | INFO -> Using ort providers: ['CUDAExecutionProvider', 'DirectMLExecutionProvider', 'OpenVINOExecutionProvider', 'ROCMExecutionProvider', 'CPUExecutionProvider', 'CoreMLExecutionProvider']\u001b[0m\n",
            "[ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/extension-node-map.json\n",
            "[ComfyUI-Manager] default cache updated: https://raw.githubusercontent.com/ltdrdata/ComfyUI-Manager/main/model-list.json\n",
            "/content/drive/MyDrive/ComfyUI/custom_nodes/comfyui_controlnet_aux/node_wrappers/dwpose.py:26: UserWarning: DWPose: Onnxruntime not found or doesn't come with acceleration providers, switch to OpenCV with CPU device. DWPose might run very slowly\n",
            "  warnings.warn(\"DWPose: Onnxruntime not found or doesn't come with acceleration providers, switch to OpenCV with CPU device. DWPose might run very slowly\")\n",
            "\u001b[34mDavemane42 Custom Nodes: \u001b[92mLoaded\u001b[0m\n",
            "\n",
            "Import times for custom nodes:\n",
            "   3.3 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/ComfyUI_Dave_CustomNode\n",
            "   6.4 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/ComfyUI-Custom-Scripts\n",
            "   7.9 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/ComfyUI-Manager\n",
            "  12.5 seconds (IMPORT FAILED): /content/drive/MyDrive/ComfyUI/custom_nodes/comfy_controlnet_preprocessors\n",
            "  22.2 seconds: /content/drive/MyDrive/ComfyUI/custom_nodes/comfyui_controlnet_aux\n",
            "\n",
            "\n",
            "ComfyUI finished loading, trying to launch localtunnel (if it gets stuck here localtunnel is having issues)\n",
            "\n",
            "The password/enpoint ip for localtunnel is: 34.125.215.134\n",
            "your url is: https://stupid-ways-allow.loca.lt\n",
            "FETCH DATA from: /content/drive/MyDrive/ComfyUI/custom_nodes/ComfyUI-Manager/extension-node-map.json\n"
          ]
        }
      ],
      "source": [
        "!npm install -g localtunnel\n",
        "\n",
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "import socket\n",
        "import urllib.request\n",
        "\n",
        "def iframe_thread(port):\n",
        "  while True:\n",
        "      time.sleep(0.5)\n",
        "      sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "      result = sock.connect_ex(('127.0.0.1', port))\n",
        "      if result == 0:\n",
        "        break\n",
        "      sock.close()\n",
        "  print(\"\\nComfyUI finished loading, trying to launch localtunnel (if it gets stuck here localtunnel is having issues)\\n\")\n",
        "\n",
        "  print(\"The password/enpoint ip for localtunnel is:\", urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))\n",
        "  p = subprocess.Popen([\"lt\", \"--port\", \"{}\".format(port)], stdout=subprocess.PIPE)\n",
        "  for line in p.stdout:\n",
        "    print(line.decode(), end='')\n",
        "\n",
        "\n",
        "threading.Thread(target=iframe_thread, daemon=True, args=(8188,)).start()\n",
        "\n",
        "!python main.py --dont-print-server"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gggggggggg"
      },
      "source": [
        "### Run ComfyUI with colab iframe (use only in case the previous way with localtunnel doesn't work)\n",
        "\n",
        "You should see the ui appear in an iframe. If you get a 403 error, it's your firefox settings or an extension that's messing things up.\n",
        "\n",
        "If you want to open it in another window use the link.\n",
        "\n",
        "Note that some UI features like live image previews won't work because the colab iframe blocks websockets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhhhhhhhhh"
      },
      "outputs": [],
      "source": [
        "import threading\n",
        "import time\n",
        "import socket\n",
        "def iframe_thread(port):\n",
        "  while True:\n",
        "      time.sleep(0.5)\n",
        "      sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "      result = sock.connect_ex(('127.0.0.1', port))\n",
        "      if result == 0:\n",
        "        break\n",
        "      sock.close()\n",
        "  from google.colab import output\n",
        "  output.serve_kernel_port_as_iframe(port, height=1024)\n",
        "  print(\"to open it in a window you can open this link here:\")\n",
        "  output.serve_kernel_port_as_window(port)\n",
        "\n",
        "threading.Thread(target=iframe_thread, daemon=True, args=(8188,)).start()\n",
        "\n",
        "!python main.py --dont-print-server"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlN7lK9tbS-i"
      },
      "source": [
        "Full Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wf1OLde0bRtZ"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English tokenizer, tagger, parser, NER, and word vectors\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Asking for user input\n",
        "ethnicities_list = [\"kyrgyz\", \"maasai\", \"newar\", \"altai\", \"bengali\", \"haitian\", \"hazara\", \"kiche\", \"lahu\",\n",
        "                    \"mizrahi\", \"quechua\", \"xhosa\", \"cree\", \"sami\", \"inuit\", \"walpiri\", \"maori\", \"basque\",\n",
        "                    \"mazigh\", \"najdi\",\"kalash\"]\n",
        "lora_dictionary={\"kyrgyz\":\"kyrgyz.safetensors\", \"maasai\":\"Masaai test Lora.safetensors\", \"newar\":\"Newar.safetensors\", \"altai\":0, \"bengali\":0, \"haitian\":0, \"hazara\":0, \"kiche\":0, \"lahu\":\"last.safetensors\",\n",
        "                    \"mizrahi\":0, \"quechua\":0, \"xhosa\":0, \"cree\":0, \"sami\":\"Sami.safetensors\", \"inuit\":0, \"walpiri\":0, \"maori\":0, \"basque\":0,\n",
        "                    \"mazigh\":0, \"najdi\":0,\"kalash\":\"Kalash_women_cluster2.safetensors\"}\n",
        "user_prompt = input(\"Please enter your text prompt: \")\n",
        "#prompts= user_prompt\n",
        "updated_sentences=[]\n",
        "lora_list=[]\n",
        "# Process the text\n",
        "doc = nlp(user_prompt)\n",
        "def is_passive(sentence):\n",
        "    doc = nlp(sentence)\n",
        "    passive = False\n",
        "    for token in doc:\n",
        "        # Check if there's a token that is a passive subject\n",
        "        if token.dep_ == 'nsubjpass':\n",
        "            passive = True\n",
        "    return passive\n",
        "def process_text(text):\n",
        "    doc = nlp(text)\n",
        "    updated_sentences = []\n",
        "    for sent in doc.sents:\n",
        "        print(f\"Sentence: {sent.text}\")\n",
        "        if is_passive(sent.text):\n",
        "          updated_sentence = passive_to_active(sent.text)\n",
        "          updated_sentences.append(updated_sentence)\n",
        "        else:\n",
        "          updated_sentence = sent.text\n",
        "          updated_sentences.append(updated_sentence)\n",
        "        for token in sent:\n",
        "          if str(token).lower() in ethnicities_list:\n",
        "              print(token)\n",
        "              lora_list.append(token)\n",
        "    updated_doc = nlp(\" \".join(updated_sentences))\n",
        "\n",
        "    return updated_doc\n",
        "# POS tagging and identifying subjects, verbs, and objects\n",
        "processed_doc = process_text(user_prompt)\n",
        "for sent in processed_doc.sents:\n",
        "    print(f\"Sentence: {sent.text}\")\n",
        "    subjects, verbs, objects = [], [], []\n",
        "    for token in sent:\n",
        "        print(f\"{token.text:10} {token.pos_:10}\")\n",
        "\n",
        "        if \"subj\" in token.dep_:\n",
        "            subjects.append(token.text)\n",
        "        if token.pos_ == \"VERB\":\n",
        "            verbs.append(token.text)\n",
        "\n",
        "        if \"obj\" in token.dep_:\n",
        "            objects.append(token.text)\n",
        "import requests\n",
        "import json\n",
        "\n",
        "# Replace 'Your_API_Key' with your actual Pixabay API key\n",
        "API_KEY = '42825189-659d2e470c25fe20af6678a1c'\n",
        "SEARCH_TERM = 'person '+verbs[0]\n",
        "\n",
        "def search_pixabay_images(query, api_key, per_page=5):\n",
        "    url = f\"https://pixabay.com/api/?key={api_key}&q={query}&image_type=photo&per_page={per_page}\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Search for images\n",
        "result = search_pixabay_images(SEARCH_TERM, API_KEY)\n",
        "\n",
        "# Print results\n",
        "if result:\n",
        "    for image in result.get('hits', []):\n",
        "        print(image['webformatURL'])\n",
        "else:\n",
        "    print(\"No results found or error in request\")\n",
        "import requests\n",
        "import os\n",
        "\n",
        "def download_and_save_image(url, folder_path, image_name):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        with open(os.path.join(folder_path, image_name), 'wb') as file:\n",
        "            file.write(response.content)\n",
        "    else:\n",
        "        print(f\"Failed to download {url}\")\n",
        "\n",
        "# Define the folder path in Google Drive\n",
        "drive_folder_path = '/content/drive/MyDrive/ControlNetImages'  # Change to your path\n",
        "os.makedirs(drive_folder_path, exist_ok=True)\n",
        "\n",
        "# Download and save each image\n",
        "if result:\n",
        "    for index, image in enumerate(result.get('hits', [])):\n",
        "        image_url = image['webformatURL']\n",
        "        image_name = f\"image_{index}.jpg\"  # Image names like image_0.jpg, image_1.jpg, etc.\n",
        "        download_and_save_image(image_url, drive_folder_path, image_name)\n",
        "else:\n",
        "    print(\"No results found or error in request\")\n",
        "import requests\n",
        "import os\n",
        "\n",
        "def get_pexels_images(api_key, query, per_page=10):\n",
        "    headers = {'Authorization': api_key}\n",
        "    params = {'query': query, 'per_page': per_page}\n",
        "    response = requests.get('https://api.pexels.com/v1/search', headers=headers, params=params)\n",
        "    return response.json()\n",
        "\n",
        "def download_image(url, folder=\"/content/drive/MyDrive/ControlNetImages\"):\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        file_path = os.path.join(folder, url.split('/')[-1])\n",
        "        with open(file_path, 'wb') as file:\n",
        "            file.write(response.content)\n",
        "        print(f\"Downloaded {file_path}\")\n",
        "\n",
        "# Replace with your actual Pexels API key\n",
        "API_KEY = 'JyAPn2giACsepLq3R7pmTli4WAJEPJtCQgAihHLzFAmAmAvnTwbelVvV'\n",
        "SEARCH_QUERY = 'person '+ verbs[0]\n",
        "\n",
        "images_data = get_pexels_images(API_KEY, SEARCH_QUERY)\n",
        "\n",
        "for photo in images_data['photos']:\n",
        "    download_image(photo['src']['original'])\n",
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the source path (Google Drive folder you want to copy) and the destination path (new folder in /content)\n",
        "source_folder = '/content/drive/MyDrive/ControlNetImages'  # Adjust this path\n",
        "destination_folder = '/content/Output'  # Adjust this folder name as needed\n",
        "\n",
        "# Create the destination folder if it doesn't exist\n",
        "if not os.path.exists(destination_folder):\n",
        "    os.makedirs(destination_folder)\n",
        "\n",
        "# Copy the entire folder from Google Drive to the new folder in /content\n",
        "for item in os.listdir(source_folder):\n",
        "    s = os.path.join(source_folder, item)\n",
        "    d = os.path.join(destination_folder, item)\n",
        "    if os.path.isdir(s):\n",
        "        shutil.copytree(s, d, False, None)\n",
        "    else:\n",
        "        shutil.copy2(s, d)\n",
        "\n",
        "print(f\"Folder '{source_folder}' has been copied to '{destination_folder}'\")\n",
        "import os\n",
        "from IPython.display import Image as IPImage\n",
        "\n",
        "# Define the path to the folder containing your images\n",
        "images_folder = \"/content/Output\"  # Update this to your images folder path\n",
        "\n",
        "# Ensure OpenPose binaries are executable\n",
        "!chmod +x /content/openpose/build/examples/openpose/openpose.bin\n",
        "\n",
        "# Define the output folder for JSON and rendered images\n",
        "output_folder = \"/content/Final\"\n",
        "os.makedirs(output_folder, exist_ok=True)  # Create output directory if it doesn't exist\n",
        "\n",
        "# Run OpenPose on the images in the specified folder\n",
        "!cd /content/openpose && ./build/examples/openpose/openpose.bin --image_dir {images_folder} --write_json {output_folder} --display 0 --write_images {output_folder}\n",
        "\n",
        "# If you want to display an example of the output, specify one of the output files\n",
        "# This is optional and just for demonstration purposes\n",
        "import os\n",
        "import json\n",
        "import shutil\n",
        "\n",
        "input_folder = '/content/Final'  # Change this to your OpenPose output folder path\n",
        "best_image_folder = '/content'  # Change this to your desired output path for the best image\n",
        "os.makedirs(best_image_folder, exist_ok=True)\n",
        "\n",
        "def score_image(json_path):\n",
        "    \"\"\"Scores an image based on keypoints and their confidence scores.\"\"\"\n",
        "    with open(json_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    score = 0\n",
        "    for person in data['people']:\n",
        "        keypoints = person['pose_keypoints_2d']\n",
        "        confidence_scores = keypoints[2::3]  # Extracting every third item as confidence score\n",
        "        score += sum(confidence_scores)  # Sum of confidence scores as the score\n",
        "    return score\n",
        "\n",
        "# Scoring images\n",
        "scores = {}\n",
        "for filename in os.listdir(input_folder):\n",
        "    if filename.endswith('.json'):\n",
        "        json_path = os.path.join(input_folder, filename)\n",
        "        scores[filename] = score_image(json_path)\n",
        "\n",
        "# Identifying the best-scored image\n",
        "best_image_json = max(scores, key=scores.get)\n",
        "best_image_base = os.path.splitext(best_image_json)[0]\n",
        "\n",
        "# Directly replacing \"_keypoints.png\" with \"_rendered.png\" in the best_image_base string\n",
        "best_image_path = os.path.join(input_folder, best_image_base.replace('_keypoints', '_rendered.png'))\n",
        "\n",
        "# Saving the best image in the specified folder\n",
        "shutil.copy(best_image_path, os.path.join(best_image_folder, os.path.basename(best_image_path)))\n",
        "\n",
        "print(f\"Best image '{os.path.basename(best_image_path)}' saved in '{best_image_folder}'\")\n",
        "import os\n",
        "import json\n",
        "import shutil\n",
        "\n",
        "input_folder = '/content/Final'  # Change this to your OpenPose output folder path\n",
        "best_image_folder = '/content'  # Change this to your desired output path for the best image\n",
        "os.makedirs(best_image_folder, exist_ok=True)\n",
        "\n",
        "def score_image(json_path):\n",
        "    \"\"\"Scores an image based on keypoints and their confidence scores.\"\"\"\n",
        "    with open(json_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    score = 0\n",
        "    for person in data['people']:\n",
        "        keypoints = person['pose_keypoints_2d']\n",
        "        confidence_scores = keypoints[2::3]  # Extracting every third item as confidence score\n",
        "        score += sum(confidence_scores)  # Sum of confidence scores as the score\n",
        "    return score\n",
        "\n",
        "# Scoring images\n",
        "scores = {}\n",
        "for filename in os.listdir(input_folder):\n",
        "    if filename.endswith('.json'):\n",
        "        json_path = os.path.join(input_folder, filename)\n",
        "        scores[filename] = score_image(json_path)\n",
        "\n",
        "# Identifying the best-scored image\n",
        "best_image_json = max(scores, key=scores.get)\n",
        "best_image_base = os.path.splitext(best_image_json)[0]\n",
        "\n",
        "# Directly replacing \"_keypoints.png\" with \"_rendered.png\" in the best_image_base string\n",
        "best_image_path = os.path.join(input_folder, best_image_base.replace('_keypoints', '_rendered.png'))\n",
        "\n",
        "# Saving the best image in the specified folder\n",
        "shutil.copy(best_image_path, os.path.join(best_image_folder, os.path.basename(best_image_path)))\n",
        "\n",
        "print(f\"Best image '{os.path.basename(best_image_path)}' saved in '{best_image_folder}'\")\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Let's assume input_folder, best_image_base, and best_image_folder are defined earlier in your code\n",
        "\n",
        "# Directly replacing \"_keypoints.png\" with \"_rendered.png\" in the best_image_base string\n",
        "best_image_path = os.path.join(input_folder, best_image_base.replace('_keypoints', '_rendered.png'))\n",
        "\n",
        "# Specify the new name for the file here\n",
        "new_filename = \"new_best_image.png\"\n",
        "\n",
        "# Saving the best image with the new name in the specified folder\n",
        "shutil.copy(best_image_path, os.path.join(best_image_folder, new_filename))\n",
        "\n",
        "print(f\"Best image '{new_filename}' saved in '{best_image_folder}'\")\n",
        "import os\n",
        "from google.colab import files\n",
        "from IPython.display import Image as IPImage\n",
        "\n",
        "# Upload an image (or use a pre-existing one)\n",
        "#uploaded = files.upload()\n",
        "image_path = \"/content/new_best_image.png\"  # Get the name of the uploaded image\n",
        "\n",
        "# Ensure OpenPose binaries are executable\n",
        "!chmod +x /content/openpose/build/examples/openpose/openpose.bin\n",
        "\n",
        "# Run OpenPose on the uploaded image\n",
        "# Adjust the parameters as needed\n",
        "output_folder = \"/content/drive/MyDrive/ControlNetImages2\"\n",
        "os.makedirs(output_folder, exist_ok=True)  # Create output directory\n",
        "!cd /content/openpose && ./build/examples/openpose/openpose.bin \\\n",
        "    --image_dir /content/ \\\n",
        "    --render_pose 1 \\\n",
        "    --disable_blending \\\n",
        "    --display 0 \\\n",
        "    --write_images {output_folder} \\\n",
        "    --write_json {output_folder}\n",
        "\n",
        "# Display the output\n",
        "output_image = os.path.join(output_folder, os.path.basename(\"new_best_image_rendered.png\"))\n",
        "display(IPImage(filename=output_image))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}